{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a8136bf",
   "metadata": {},
   "source": [
    "#### Second iteration used clean engineered dataset from 2025 global jobs trends obtained from Kaggle on 10/06/2025. \n",
    "-- This is the seoncd iteration for Descriptive stats and EDA for the dataa being used to model precedent based causality for salary and category of jobs globally, and by country \n",
    "-- \"How does salary change for an increase in avaailable jobs by category globally (by country). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af86033b",
   "metadata": {},
   "source": [
    "### import libraries, load data, data review\n",
    "### What is the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14151f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset with CSV first, then Excel fallback\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "csv_path = 'df_clean2.csv'\n",
    "#excel_path = 'dfall_clean.xlsx'\n",
    "\n",
    "def load_data():\n",
    "    if os.path.exists(csv_path):\n",
    "        print(f\"Found {csv_path}, loading CSV...\")\n",
    "        df_clean2 = pd.read_csv(csv_path)\n",
    "    #elif os.path.exists(excel_path):\n",
    "        #print(f\"CSV not found. Found {excel_path}, loading Excel...\")\n",
    "        #df = pd.read_excel(excel_path)\n",
    "    #else:\n",
    "        #raise FileNotFoundError(f\"Neither {csv_path} nor {excel_path} were found in the current directory.\")\n",
    "    print('Dataframe shape:', df_clean2.shape)\n",
    "    display(df_clean2.head())\n",
    "    return df_clean2\n",
    "\n",
    "# Execute load\n",
    "df = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9284c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os, re, datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462ee3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick verification of loaded dataframe\n",
    "print('DataFrame info:')\n",
    "print('-'*40)\n",
    "df.info()\n",
    "print('\\nFirst 5 rows:')\n",
    "print('-'*40)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0742c3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "file_name = \"global_job_trend_clean.csv\"\n",
    "if not os.path.exists(file_name):\n",
    "    print(f\"File not found: {file_name}. Current working directory: {os.getcwd()}\")\n",
    "else:\n",
    "    df = pd.read_csv(file_name)\n",
    "    print(f\"Loaded: {file_name}\")\n",
    "    print(\"Shape:\", df.shape)\n",
    "    print(\"Head:\")\n",
    "    print(df.head())\n",
    "\n",
    "    # Understand data\n",
    "    print(\"\\nDataFrame info:\")\n",
    "    df.info()\n",
    "    print(\"\\nDimensions (ndim):\", df.ndim)\n",
    "    print(\"Shape:\", df.shape)\n",
    "    print(\"Column names:\", df.columns.tolist())\n",
    "    print(\"Data types:\\n\", df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffef4fe9",
   "metadata": {},
   "source": [
    "## Make intial data tyes updates Dates to date_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe92704e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert columns containing '_date_' to datetime where appropriate (skip date-part integers)\n",
    "date_like_cols = [c for c in df.columns if '_date_' in c]\n",
    "\n",
    "part_suffixes = ('_year', '_month', '_day', '_iso_year', '_iso_week')\n",
    "converted, skipped = [], []\n",
    "\n",
    "for col in date_like_cols:\n",
    "    cname = col.lower()\n",
    "    if cname.endswith(part_suffixes):\n",
    "        skipped.append(col)  # keep date-part integers as-is\n",
    "        continue\n",
    "    if pd.api.types.is_datetime64_any_dtype(df[col]):\n",
    "        skipped.append(col)  # already datetime\n",
    "        continue\n",
    "    if pd.api.types.is_numeric_dtype(df[col]):\n",
    "        skipped.append(col)  # avoid converting numeric parts to epoch dates\n",
    "        continue\n",
    "    df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "    converted.append(col)\n",
    "\n",
    "print(f\"Converted to datetime: {converted if converted else 'None'}\")\n",
    "print(f\"Skipped (parts/already datetime/numeric): {skipped if skipped else 'None'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e735a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b139bcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all object dtype columns in df to category\n",
    "obj_cols = df.select_dtypes(include=\"object\").columns.tolist()\n",
    "\n",
    "if not obj_cols:\n",
    "    print(\"No object columns to convert.\")\n",
    "else:\n",
    "    df[obj_cols] = df[obj_cols].apply(lambda s: s.astype(\"category\"))\n",
    "    print(f\"Converted object -> category for {len(obj_cols)} columns:\")\n",
    "    print(obj_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e91ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aea8e43",
   "metadata": {},
   "source": [
    "## Observe missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe64295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Check for missing values in all columns\n",
    "missing_summary = df.isna().sum().sort_values(ascending=False)\n",
    "print(\"Missing values in key columns:\")\n",
    "print(missing_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7eb057",
   "metadata": {},
   "source": [
    "## Start Decsriptive statistics, summary and visual explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8280798",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567b775e",
   "metadata": {},
   "source": [
    "### Summarize missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4104501b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Check for missing values in key columns\n",
    "missing_summary = df.isnull().sum()\n",
    "print(\"Missing values in columns:\")\n",
    "print(missing_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484eaec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing data: counts per column and missingness heatmap\n",
    "\n",
    "# Use existing summary if available, otherwise compute\n",
    "_missing_counts = missing_summary.sort_values(ascending=False) if 'missing_summary' in globals() else df.isna().sum().sort_values(ascending=False)\n",
    "_missing_pct = (_missing_counts / len(df) * 100).round(2)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6), constrained_layout=True)\n",
    "\n",
    "# Barplot of missing counts (with % annotations)\n",
    "sns.barplot(x=_missing_counts.values, y=_missing_counts.index, ax=axes[0], palette='viridis')\n",
    "axes[0].set_title('Missing values by column')\n",
    "axes[0].set_xlabel('Missing count')\n",
    "axes[0].set_ylabel('')\n",
    "for i, (count, pct) in enumerate(zip(_missing_counts.values, _missing_pct.values)):\n",
    "    axes[0].text(count, i, f'  {count} ({pct}%)', va='center')\n",
    "\n",
    "# Heatmap of missingness pattern (first N rows for readability)\n",
    "_n = min(600, len(df))\n",
    "sns.heatmap(df.head(_n).isna(), ax=axes[1], cbar=False, yticklabels=False, cmap='viridis')\n",
    "axes[1].set_title(f'Missingness pattern (first {_n} rows)')\n",
    "axes[1].set_xlabel('Columns')\n",
    "axes[1].set_ylabel('Rows')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcd73cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fff5dac",
   "metadata": {},
   "source": [
    "### Look at the 5 momnets for undestanding data quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6f4730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distributions: histograms for numeric, bar charts for categorical\n",
    "import math\n",
    "import matplotlib as mpl\n",
    "\n",
    "# Reduce font sizes and increase padding to avoid overlapping text in subplots\n",
    "mpl.rcParams.update({\n",
    "    \"axes.titlesize\": 10,\n",
    "    \"xtick.labelsize\": 8,\n",
    "    \"ytick.labelsize\": 8,\n",
    "    \"figure.titlesize\": 12,\n",
    "    \"axes.titlepad\": 6,\n",
    "    # Use constrained layout with extra breathing room\n",
    "    \"figure.constrained_layout.use\": True,\n",
    "    \"figure.constrained_layout.w_pad\": 6.0 / 72.0,\n",
    "    \"figure.constrained_layout.h_pad\": 6.0 / 72.0,\n",
    "    \"figure.constrained_layout.wspace\": 0.10,\n",
    "    \"figure.constrained_layout.hspace\": 0.10,\n",
    "})\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1) Numeric variable histograms\n",
    "num_cols = df.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "if num_cols:\n",
    "    n = len(num_cols)\n",
    "    cols = 3 if n >= 3 else n\n",
    "    rows = math.ceil(n / cols)\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols * 5.2, rows * 3.6), constrained_layout=True)\n",
    "    axes = axes.flatten() if n > 1 else [axes]\n",
    "\n",
    "    for i, col in enumerate(num_cols):\n",
    "        sns.histplot(df[col].dropna(), bins=30, kde=True, ax=axes[i], color=\"steelblue\")\n",
    "        axes[i].set_title(str(col))\n",
    "        axes[i].set_xlabel(\"\")\n",
    "        axes[i].set_ylabel(\"Count\")\n",
    "\n",
    "    # Hide any unused axes\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis(\"off\")\n",
    "\n",
    "    fig.suptitle(\"Numeric distributions (hist + KDE)\", fontsize=14)\n",
    "    fig.savefig(\"histograms_numeric.png\", dpi=220, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(f\"Saved numeric histograms to histograms_numeric.png ({n} features).\")\n",
    "else:\n",
    "    print(\"No numeric columns found for histograms.\")\n",
    "\n",
    "# 2) Categorical variable bar charts (top 20 levels per column)\n",
    "cat_cols = df.select_dtypes(include=[\"category\", \"object\", \"bool\"]).columns.tolist()\n",
    "if cat_cols:\n",
    "    n = len(cat_cols)\n",
    "    cols = 3 if n >= 3 else n\n",
    "    rows = math.ceil(n / cols)\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols * 6.0, rows * 4.2), constrained_layout=True)\n",
    "    axes = axes.flatten() if n > 1 else [axes]\n",
    "\n",
    "    for i, col in enumerate(cat_cols):\n",
    "        vc = df[col].astype(str).value_counts(dropna=False).head(20)\n",
    "        sns.barplot(x=vc.values, y=vc.index, ax=axes[i], palette=\"viridis\")\n",
    "        axes[i].set_title(f\"{col} (top 20)\")\n",
    "        axes[i].set_xlabel(\"Count\")\n",
    "        axes[i].set_ylabel(\"\")\n",
    "\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis(\"off\")\n",
    "\n",
    "    fig.suptitle(\"Categorical distributions (top 20 levels)\", fontsize=14)\n",
    "    fig.savefig(\"barplots_categorical.png\", dpi=220, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(f\"Saved categorical bar plots to barplots_categorical.png ({n} features).\")\n",
    "else:\n",
    "    print(\"No categorical/object/bool columns found for bar plots.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21236a96",
   "metadata": {},
   "source": [
    "### Data Spread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396ea119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data spread exploration: summary stats, boxplots for numeric columns, and salary spread by key categories\n",
    "\n",
    "# 1) Summary of spread for numeric columns (use existing num_cols if available)\n",
    "_s_num_cols = [c for c in num_cols if c != 'id'] if 'num_cols' in globals() else df.select_dtypes(include=['number']).columns.tolist()\n",
    "if not _s_num_cols:\n",
    "    print(\"No numeric columns found for spread analysis.\")\n",
    "else:\n",
    "    _rows = []\n",
    "    for _c in _s_num_cols:\n",
    "        _s = pd.to_numeric(df[_c], errors='coerce').dropna()\n",
    "        if _s.empty:\n",
    "            continue\n",
    "        _q1 = _s.quantile(0.25)\n",
    "        _q2 = _s.quantile(0.50)\n",
    "        _q3 = _s.quantile(0.75)\n",
    "        _iqr = _q3 - _q1\n",
    "        _mad = float(np.median(np.abs(_s - _q2)))\n",
    "        _cv = float(_s.std(ddof=1) / _s.mean()) if _s.mean() != 0 else np.nan\n",
    "        _lo = _q1 - 1.5 * _iqr\n",
    "        _hi = _q3 + 1.5 * _iqr\n",
    "        _out = int(((_s < _lo) | (_s > _hi)).sum())\n",
    "        _rows.append({\n",
    "            \"column\": _c,\n",
    "            \"count\": int(_s.size),\n",
    "            \"mean\": float(_s.mean()),\n",
    "            \"std\": float(_s.std(ddof=1)),\n",
    "            \"var\": float(_s.var(ddof=1)),\n",
    "            \"min\": float(_s.min()),\n",
    "            \"q1\": float(_q1),\n",
    "            \"median\": float(_q2),\n",
    "            \"q3\": float(_q3),\n",
    "            \"iqr\": float(_iqr),\n",
    "            \"max\": float(_s.max()),\n",
    "            \"mad_median\": _mad,\n",
    "            \"cv\": _cv,\n",
    "            \"outliers_iqr_1_5x\": _out,\n",
    "        })\n",
    "    spread_summary = pd.DataFrame(_rows).set_index(\"column\").sort_index()\n",
    "    print(\"Numeric spread summary:\")\n",
    "    display(spread_summary)\n",
    "\n",
    "    # 2) Boxplots for numeric columns (except id)\n",
    "    _plot_cols = _s_num_cols\n",
    "    _ncols = 3 if len(_plot_cols) >= 3 else len(_plot_cols)\n",
    "    _nrows = (len(_plot_cols) + _ncols - 1) // _ncols if _ncols else 0\n",
    "    if _nrows > 0:\n",
    "        fig_bp, axs_bp = plt.subplots(_nrows, _ncols, figsize=(_ncols * 5.4, _nrows * 3.8), constrained_layout=True)\n",
    "        axs_bp = axs_bp.flatten() if isinstance(axs_bp, np.ndarray) else [axs_bp]\n",
    "        for _i, _c in enumerate(_plot_cols):\n",
    "            sns.boxplot(x=df[_c], ax=axs_bp[_i], color=\"steelblue\", whis=1.5, fliersize=2)\n",
    "            axs_bp[_i].set_title(f\"{_c} — boxplot\")\n",
    "            axs_bp[_i].set_xlabel(\"\")\n",
    "            axs_bp[_i].set_xlabel(_c)\n",
    "        for _j in range(_i + 1, len(axs_bp)):\n",
    "            axs_bp[_j].axis(\"off\")\n",
    "        fig_bp.suptitle(\"Spread of numeric features — boxplots\", fontsize=13)\n",
    "        fig_bp.savefig(\"spread_boxplots_numeric.png\", dpi=220, bbox_inches=\"tight\")\n",
    "        plt.show()\n",
    "        print(\"Saved spread_boxplots_numeric.png\")\n",
    "\n",
    "# 3) Salary spread by key categories (top levels)\n",
    "if 'avg_salary' in df.columns:\n",
    "    _group_candidates = []\n",
    "    if 'cat_cols' in globals():\n",
    "        _group_candidates = [c for c in ['country', 'category_combined_clean'] if c in cat_cols]\n",
    "    else:\n",
    "        for _cand in ['country', 'category_combined_clean']:\n",
    "            if _cand in df.columns:\n",
    "                _group_candidates.append(_cand)\n",
    "\n",
    "    _top_k = 12\n",
    "    for _gcol in _group_candidates:\n",
    "        _mask = df['avg_salary'].notna() & df[_gcol].notna()\n",
    "        if not _mask.any():\n",
    "            continue\n",
    "        _order = df.loc[_mask, _gcol].value_counts().head(_top_k).index.tolist()\n",
    "        if not _order:\n",
    "            continue\n",
    "        _sub = df.loc[_mask & df[_gcol].isin(_order), ['avg_salary', _gcol]].copy()\n",
    "\n",
    "        plt.figure(figsize=(9, 0.5 * len(_order) + 2))\n",
    "        sns.boxplot(\n",
    "            data=_sub,\n",
    "            x='avg_salary',\n",
    "            y=_gcol,\n",
    "            order=_order,\n",
    "            showfliers=False,\n",
    "            palette='viridis'\n",
    "        )\n",
    "        plt.title(f\"avg_salary spread by {_gcol} (top {_top_k})\")\n",
    "        plt.xlabel(\"avg_salary\")\n",
    "        plt.ylabel(_gcol)\n",
    "        _out_file = f\"spread_avg_salary_by_{_gcol}.png\"\n",
    "        plt.savefig(_out_file, dpi=220, bbox_inches=\"tight\")\n",
    "        plt.show()\n",
    "        print(f\"Saved {_out_file}\")\n",
    "\n",
    "        # Optional: per-group IQR table\n",
    "        def _iqr_fn(s):\n",
    "            return s.quantile(0.75) - s.quantile(0.25)\n",
    "        _grp_stats = _sub.groupby(_gcol)['avg_salary'].agg(\n",
    "            count='size', mean='mean', std='std', q1=lambda s: s.quantile(0.25),\n",
    "            median='median', q3=lambda s: s.quantile(0.75), iqr=_iqr_fn\n",
    "        ).loc[_order]\n",
    "        print(f\"avg_salary spread by '{_gcol}' (top {_top_k}) — summary:\")\n",
    "        display(_grp_stats.round(2))\n",
    "else:\n",
    "    print(\"'avg_salary' not found; skipping salary-by-category spread.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d9d126",
   "metadata": {},
   "source": [
    "### Kurtosis and extreme values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5104901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skewness and kurtosis for avg_salary only\n",
    "# Override numeric columns list to just ['avg_salary']\n",
    "num_cols = ['avg_salary'] if 'avg_salary' in df.columns else []\n",
    "\n",
    "# Use existing numeric columns list if available; otherwise detect\n",
    "_num_cols = num_cols if 'num_cols' in globals() else df.select_dtypes(include=['number']).columns.tolist()\n",
    "if not _num_cols:\n",
    "    print(\"No numeric columns found for skewness/kurtosis.\")\n",
    "else:\n",
    "    X = df[_num_cols].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    skew = X.skew()\n",
    "    kurt_excess = X.kurt()           # Fisher definition: Normal => 0\n",
    "    counts = X.count()\n",
    "\n",
    "    skew_kurt_summary = (\n",
    "        pd.DataFrame({\n",
    "            \"count\": counts,\n",
    "            \"skewness\": skew,\n",
    "            \"excess_kurtosis\": kurt_excess,\n",
    "            \"pearson_kurtosis\": kurt_excess + 3,  # Normal => 3\n",
    "        })\n",
    "        .assign(abs_skew=lambda d: d[\"skewness\"].abs(),\n",
    "                abs_excess_kurtosis=lambda d: d[\"excess_kurtosis\"].abs())\n",
    "        .sort_values(\"abs_skew\", ascending=False)\n",
    "    )\n",
    "\n",
    "    print(\"Skewness and kurtosis summary (sorted by |skew|):\")\n",
    "    display(skew_kurt_summary)\n",
    "\n",
    "    # Bar plots for top features by |skew| and |excess kurtosis|\n",
    "    top_k = 12\n",
    "    top_sk = skew_kurt_summary.head(top_k).iloc[::-1]  # reverse for nicer barh order\n",
    "    top_kt = (\n",
    "        skew_kurt_summary.sort_values(\"abs_excess_kurtosis\", ascending=False)\n",
    "        .head(top_k)\n",
    "        .iloc[::-1]\n",
    "    )\n",
    "\n",
    "    fig_sk, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, max(5, 0.5 * top_k)), constrained_layout=True)\n",
    "\n",
    "    sns.barplot(x=top_sk[\"skewness\"], y=top_sk.index, ax=ax1, palette=\"coolwarm\")\n",
    "    ax1.axvline(0, color=\"k\", lw=1)\n",
    "    ax1.set_title(\"Skewness (top by |skew|)\")\n",
    "    ax1.set_xlabel(\"Skewness\")\n",
    "    ax1.set_ylabel(\"\")\n",
    "\n",
    "    sns.barplot(x=top_kt[\"excess_kurtosis\"], y=top_kt.index, ax=ax2, palette=\"magma\")\n",
    "    ax2.axvline(0, color=\"k\", lw=1)\n",
    "    ax2.set_title(\"Excess kurtosis (top by |excess|)\")\n",
    "    ax2.set_xlabel(\"Excess kurtosis\")\n",
    "    ax2.set_ylabel(\"\")\n",
    "\n",
    "    out_png = \"skew_kurtosis_bars.png\"\n",
    "    fig_sk.savefig(out_png, dpi=220, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(f\"Saved {out_png}\")\n",
    "\n",
    "    # Optional: save table\n",
    "    out_csv = \"skew_kurtosis_summary.csv\"\n",
    "    skew_kurt_summary.to_csv(out_csv, index=True)\n",
    "    print(f\"Saved {out_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75947b64",
   "metadata": {},
   "source": [
    "### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c096a86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix heatmap for numeric features\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Select numeric columns only\n",
    "num_cols = df.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "if len(num_cols) < 2:\n",
    "    print(\"Not enough numeric columns to compute a correlation matrix.\")\n",
    "else:\n",
    "    corr = df[num_cols].corr()\n",
    "\n",
    "    # Dynamic figure size based on number of columns\n",
    "    n = len(num_cols)\n",
    "    size = max(8, min(0.55 * n + 4, 20))\n",
    "\n",
    "    # Mask the upper triangle to reduce clutter\n",
    "    mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "    plt.figure(figsize=(size, size))\n",
    "    sns.heatmap(\n",
    "        corr,\n",
    "        mask=mask,\n",
    "        cmap=\"RdBu_r\",\n",
    "        center=0,\n",
    "        annot=(n <= 15),   # annotate only if manageable number of vars\n",
    "        fmt=\".2f\",\n",
    "        square=True,\n",
    "        linewidths=0.5,\n",
    "        cbar_kws={\"shrink\": 0.8, \"label\": \"Pearson r\"}\n",
    "    )\n",
    "    plt.title(\"Correlation matrix (numeric features)\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.yticks(rotation=0)\n",
    "    # Using constrained layout via rcParams; avoid tight_layout to prevent layout-engine conflict with colorbar.\n",
    "\n",
    "    # Save and display\n",
    "    plt.savefig(\"corr_matrix.png\", dpi=220, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Saved heatmap to corr_matrix.png with {n} numeric features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e731d7",
   "metadata": {},
   "source": [
    "### Scatter plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f1816b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairwise x-y scatter matrices with regression lines (no duplicates)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Select numeric columns\n",
    "num_cols = df.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "\n",
    "if len(num_cols) < 2:\n",
    "    print(\"Not enough numeric columns for a pairwise scatter matrix (need >= 2).\")\n",
    "else:\n",
    "    # Limit number of variables for readability/performance (choose by variance)\n",
    "    max_vars = 10  # adjust if you want more/less\n",
    "    if len(num_cols) > max_vars:\n",
    "        var_order = df[num_cols].var(numeric_only=True).sort_values(ascending=False)\n",
    "        selected_cols = var_order.index[:max_vars].tolist()\n",
    "        print(f\"Selected top {max_vars} numeric columns by variance for plotting:\\n{selected_cols}\")\n",
    "    else:\n",
    "        selected_cols = num_cols\n",
    "\n",
    "    # Optional row sampling for very large datasets\n",
    "    max_rows = 5000\n",
    "    if len(df) > max_rows:\n",
    "        dplot = df[selected_cols].sample(max_rows, random_state=42)\n",
    "        print(f\"Sampled {max_rows} rows from {len(df)} for speed.\")\n",
    "    else:\n",
    "        dplot = df[selected_cols]\n",
    "\n",
    "    # Build pairplot with regression in lower triangle only (no duplicates)\n",
    "    sns.set_theme()\n",
    "    g = sns.pairplot(\n",
    "        dplot,\n",
    "        kind=\"reg\",\n",
    "        diag_kind=\"kde\",\n",
    "        corner=True,              # show only lower triangle (no duplicates)\n",
    "        plot_kws={\n",
    "            \"scatter_kws\": {\"alpha\": 0.35, \"s\": 16, \"edgecolor\": \"none\"},\n",
    "            \"line_kws\": {\"color\": \"crimson\", \"lw\": 1.5},\n",
    "            \"ci\": 95,\n",
    "        },\n",
    "        height=2.0,               # size of each subplot\n",
    "        aspect=1.0,\n",
    "    )\n",
    "\n",
    "    # Tweak labels/spacing and save\n",
    "    g.fig.suptitle(\n",
    "        f\"Pairwise scatter with regression (lower triangle) — {len(selected_cols)} vars\",\n",
    "        y=1.02,\n",
    "        fontsize=13,\n",
    "    )\n",
    "    g.fig.tight_layout()\n",
    "    g.fig.savefig(\"scatter_matrix_reg.png\", dpi=220, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(\"Saved scatterplot matrix to scatter_matrix_reg.png.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81e0dba",
   "metadata": {},
   "source": [
    "### categorical count and propotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83135e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical counts and proportions + bar/pie visualization\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Config\n",
    "viz_type = \"bar\"   # choose: \"bar\" or \"pie\"\n",
    "top_n = 10          # show top N categories per column\n",
    "include_other = True\n",
    "\n",
    "# Identify categorical-like columns\n",
    "cat_cols = df.select_dtypes(include=[\"category\", \"object\", \"bool\"]).columns.tolist()\n",
    "cat_cols = [c for c in cat_cols if df[c].notna().sum() > 0]\n",
    "\n",
    "if not cat_cols:\n",
    "    print(\"No categorical/object/bool columns found.\")\n",
    "else:\n",
    "    # Build tidy summary table: column, category, count, proportion_pct\n",
    "    def summarize_categoricals(frame, columns, top=10, add_other=True):\n",
    "        parts = []\n",
    "        for col in columns:\n",
    "            s = frame[col].astype(\"string\").fillna(\"<NA>\")\n",
    "            vc = s.value_counts(dropna=False)\n",
    "            total = int(vc.sum())\n",
    "            take = vc.head(top)\n",
    "            if add_other and len(vc) > top:\n",
    "                other_count = int(total - int(take.sum()))\n",
    "                take.loc[\"Other\"] = other_count\n",
    "            part = pd.DataFrame({\n",
    "                \"column\": col,\n",
    "                \"category\": take.index,\n",
    "                \"count\": take.values,\n",
    "            })\n",
    "            part[\"proportion_pct\"] = (part[\"count\"] / total * 100.0).round(2)\n",
    "            parts.append(part)\n",
    "        return pd.concat(parts, ignore_index=True)\n",
    "\n",
    "    cat_summary = summarize_categoricals(df, cat_cols, top=top_n, add_other=include_other)\n",
    "    display(cat_summary.head(30))\n",
    "\n",
    "    # Plot grid\n",
    "    n = len(cat_cols)\n",
    "    cols = 3 if n >= 3 else n\n",
    "    rows = math.ceil(n / cols)\n",
    "    figsize = (cols * 6.2, rows * (4.2 if viz_type == \"bar\" else 4.8))\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=figsize, constrained_layout=True)\n",
    "    axes = axes.flatten() if n > 1 else [axes]\n",
    "\n",
    "    for i, col in enumerate(cat_cols):\n",
    "        data = cat_summary[cat_summary[\"column\"] == col].copy()\n",
    "        data = data.sort_values(\"count\", ascending=False)\n",
    "\n",
    "        if viz_type == \"bar\":\n",
    "            sns.barplot(x=\"count\", y=\"category\", data=data, ax=axes[i], palette=\"viridis\")\n",
    "            axes[i].set_title(f\"{col} — top {min(top_n, len(data))}\")\n",
    "            axes[i].set_xlabel(\"Count\")\n",
    "            axes[i].set_ylabel(\"\")\n",
    "            # annotate with percentage\n",
    "            for p, pct in zip(axes[i].patches, data[\"proportion_pct\"].values):\n",
    "                width = p.get_width()\n",
    "                y = p.get_y() + p.get_height() / 2\n",
    "                axes[i].text(width, y, f\"  {int(width)} ({pct:.1f}%)\", va=\"center\")\n",
    "        else:  # pie\n",
    "            counts = data[\"count\"].values\n",
    "            labels = data[\"category\"].astype(str).values\n",
    "            total = counts.sum()\n",
    "            def autopct_fmt(p):\n",
    "                count = int(round(p / 100.0 * total))\n",
    "                return f\"{p:.1f}%\\n({count})\"\n",
    "            axes[i].pie(\n",
    "                counts,\n",
    "                labels=labels,\n",
    "                autopct=autopct_fmt,\n",
    "                startangle=140,\n",
    "                textprops={\"fontsize\": 8},\n",
    "                colors=sns.color_palette(\"viridis\", n_colors=len(counts)),\n",
    "            )\n",
    "            axes[i].axis(\"equal\")\n",
    "            axes[i].set_title(f\"{col} — top {min(top_n, len(data))}\")\n",
    "\n",
    "    # Hide unused axes\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis(\"off\")\n",
    "\n",
    "    fig.suptitle(\n",
    "        f\"Categorical distributions (top {top_n}) — {viz_type.upper()}\\nCounts annotated with %\",\n",
    "        fontsize=14,\n",
    "    )\n",
    "\n",
    "    out_name = f\"categorical_counts_proportions_{viz_type}.png\"\n",
    "    fig.savefig(out_name, dpi=220, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(f\"Saved figure to {out_name} for {n} categorical columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de601cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffd16b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1918fe",
   "metadata": {},
   "source": [
    "### Go to file for global job trends .ipynb data descriptive stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b0859e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
